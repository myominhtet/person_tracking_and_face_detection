{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"private_outputs":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","source":["!pip install fastreid\n","!pip install mtcnn"],"metadata":{"id":"T1ay99nqJ0Y5"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5OGvVvDb6b-w"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import sys\n","import os \n","import time, datetime, random\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import matplotlib.patches as patches\n","from PIL import Image\n","import torch\n","import torchvision\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms\n","from torch.autograd import Variable"],"metadata":{"id":"1xwDT9sx7wa9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sys.path.append('/content/drive/MyDrive/PyTorch-YOLOv3-master')\n","sys.path.append('/content/drive/MyDrive/deep_sort_pytorch-master')"],"metadata":{"id":"JDPZYmKf8GtD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pytorchyolo.models import*\n","from pytorchyolo.utils import*\n","from pytorchyolo.detect import*\n","config_path = '/content/drive/MyDrive/PyTorch-YOLOv3-master/config/yolov3.cfg'\n","weight_path = '/content/drive/MyDrive/3.weights'\n","class_path='/content/drive/MyDrive/PyTorch-YOLOv3-master/data/coco.names'\n","img_size=416\n","conf_thres=0.8\n","nms_thres=0.4"],"metadata":{"id":"XQZ74vzg7-po"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#load model and weights\n","model = Darknet(config_path)\n","model.load_darknet_weights(weight_path)\n","model.cuda()\n","model.eval()\n","classes=utils.load_classes(class_path)\n","Tensor = torch.cuda.FloatTensor"],"metadata":{"id":"bBg9IAnO8DmF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import dependencies\n","from IPython.display import display, Javascript, Image\n","from google.colab.output import eval_js\n","from base64 import b64decode, b64encode\n","import cv2\n","import numpy as np\n","import PIL\n","import io\n","import html\n","import time"],"metadata":{"id":"F5_jHCxrE30G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# function to convert the JavaScript object into an OpenCV image\n","def js_to_image(js_reply):\n","  \"\"\"\n","  Params:\n","          js_reply: JavaScript object containing image from webcam\n","  Returns:\n","          img: OpenCV BGR image\n","  \"\"\"\n","  # decode base64 image\n","  image_bytes = b64decode(js_reply.split(',')[1])\n","  # convert bytes to numpy array\n","  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n","  # decode numpy array into OpenCV BGR image\n","  img = cv2.imdecode(jpg_as_np, flags=1)\n","\n","  return img\n","\n","# function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream\n","def bbox_to_bytes(bbox_array):\n","  \"\"\"\n","  Params:\n","          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.\n","  Returns:\n","        bytes: Base64 image byte string\n","  \"\"\"\n","  # convert array into PIL image\n","  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n","  iobuf = io.BytesIO()\n","  # format bbox into png for return\n","  bbox_PIL.save(iobuf, format='png')\n","  # format return string\n","  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n","\n","  return bbox_bytes"],"metadata":{"id":"K9EjAlXpFAXp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# JavaScript to properly create our live video stream using our webcam as input\n","def video_stream():\n","  js = Javascript('''\n","    var video;\n","    var div = null;\n","    var stream;\n","    var captureCanvas;\n","    var imgElement;\n","    var labelElement;\n","    \n","    var pendingResolve = null;\n","    var shutdown = false;\n","    \n","    function removeDom() {\n","       stream.getVideoTracks()[0].stop();\n","       video.remove();\n","       div.remove();\n","       video = null;\n","       div = null;\n","       stream = null;\n","       imgElement = null;\n","       captureCanvas = null;\n","       labelElement = null;\n","    }\n","    \n","    function onAnimationFrame() {\n","      if (!shutdown) {\n","        window.requestAnimationFrame(onAnimationFrame);\n","      }\n","      if (pendingResolve) {\n","        var result = \"\";\n","        if (!shutdown) {\n","          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n","          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n","        }\n","        var lp = pendingResolve;\n","        pendingResolve = null;\n","        lp(result);\n","      }\n","    }\n","    \n","    async function createDom() {\n","      if (div !== null) {\n","        return stream;\n","      }\n","\n","      div = document.createElement('div');\n","      div.style.border = '2px solid black';\n","      div.style.padding = '3px';\n","      div.style.width = '100%';\n","      div.style.maxWidth = '600px';\n","      document.body.appendChild(div);\n","      \n","      const modelOut = document.createElement('div');\n","      modelOut.innerHTML = \"<span>Status:</span>\";\n","      labelElement = document.createElement('span');\n","      labelElement.innerText = 'No data';\n","      labelElement.style.fontWeight = 'bold';\n","      modelOut.appendChild(labelElement);\n","      div.appendChild(modelOut);\n","           \n","      video = document.createElement('video');\n","      video.style.display = 'block';\n","      video.width = div.clientWidth - 6;\n","      video.setAttribute('playsinline', '');\n","      video.onclick = () => { shutdown = true; };\n","      stream = await navigator.mediaDevices.getUserMedia(\n","          {video: { facingMode: \"environment\"}});\n","      div.appendChild(video);\n","\n","      imgElement = document.createElement('img');\n","      imgElement.style.position = 'absolute';\n","      imgElement.style.zIndex = 1;\n","      imgElement.onclick = () => { shutdown = true; };\n","      div.appendChild(imgElement);\n","      \n","      const instruction = document.createElement('div');\n","      instruction.innerHTML = \n","          '<span style=\"color: red; font-weight: bold;\">' +\n","          'When finished, click here or on the video to stop this demo</span>';\n","      div.appendChild(instruction);\n","      instruction.onclick = () => { shutdown = true; };\n","      \n","      video.srcObject = stream;\n","      await video.play();\n","\n","      captureCanvas = document.createElement('canvas');\n","      captureCanvas.width = 640; //video.videoWidth;\n","      captureCanvas.height = 480; //video.videoHeight;\n","      window.requestAnimationFrame(onAnimationFrame);\n","      \n","      return stream;\n","    }\n","    async function stream_frame(label, imgData) {\n","      if (shutdown) {\n","        removeDom();\n","        shutdown = false;\n","        return '';\n","      }\n","\n","      var preCreate = Date.now();\n","      stream = await createDom();\n","      \n","      var preShow = Date.now();\n","      if (label != \"\") {\n","        labelElement.innerHTML = label;\n","      }\n","            \n","      if (imgData != \"\") {\n","        var videoRect = video.getClientRects()[0];\n","        imgElement.style.top = videoRect.top + \"px\";\n","        imgElement.style.left = videoRect.left + \"px\";\n","        imgElement.style.width = videoRect.width + \"px\";\n","        imgElement.style.height = videoRect.height + \"px\";\n","        imgElement.src = imgData;\n","      }\n","      \n","      var preCapture = Date.now();\n","      var result = await new Promise(function(resolve, reject) {\n","        pendingResolve = resolve;\n","      });\n","      shutdown = false;\n","      \n","      return {'create': preShow - preCreate, \n","              'show': preCapture - preShow, \n","              'capture': Date.now() - preCapture,\n","              'img': result};\n","    }\n","    ''')\n","\n","  display(js)\n","  \n","def video_frame(label, bbox):\n","  data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n","  return data"],"metadata":{"id":"UQuRvBY1IXzx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#feature extraction\n","sys.path.append('/content/drive/MyDrive')\n","from feature_extraction import extract_features"],"metadata":{"id":"6yAKRquMR26S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pytorchyolo.detect import*\n","from deep_sort.sort.detection import*\n","from deep_sort.sort.tracker import Tracker\n","from deep_sort.sort.kalman_filter import KalmanFilter\n","from deep_sort.sort.nn_matching import NearestNeighborDistanceMetric\n","from deep_sort import DeepSort\n","import cv2\n","from google.colab.patches import cv2_imshow\n","from mtcnn import MTCNN\n","\n","# Create an instance of the MTCNN model\n","face_detector = MTCNN()\n","\n","# Create VideoWriter object to save the processed frames as a video\n","fourcc = cv2.VideoWriter_fourcc(*'XVID')\n","out = cv2.VideoWriter('output.avi', fourcc, 2.0, (640, 480))\n","\n","# metric\n","metric = NearestNeighborDistanceMetric(\"cosine\", 0.2, 100)\n","tracker = Tracker(metric, max_iou_distance=0.7, max_age=70, n_init=3)\n","\n","# start streaming video from webcam\n","video_stream()\n","\n","# label for video\n","label_html = 'Capturing...'\n","\n","# initialize bounding box to empty\n","bbox = ''\n","count = 0\n","\n","while True:\n","    js_reply = video_frame(label_html, bbox)\n","    if not js_reply:\n","        break\n","\n","    # convert JS response to OpenCV Image\n","    img = js_to_image(js_reply[\"img\"])\n","    input_img = cv2.resize(img, (416, 416))\n","\n","    #convert image to rgb for face detection\n","    image_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","\n","    # Detect faces in the image\n","    faces = face_detector.detect_faces(image_rgb)\n","    \n","    # yolo detection\n","    detection = []\n","    detections = detect_image(model, img, img_size=416, conf_thres=0.5, nms_thres=0.5)\n","    for x1, y1, x2, y2, conf, cls_pred in detections:\n","      if classes[int(cls_pred)] == 'person':\n","        # extract features from an using roi and cnn model\n","        image_pil = Image.fromarray(cv2.cvtColor(input_img, cv2.COLOR_BGR2RGB))\n","        features = extract_features(image_pil, (x1, y1, x2, y2))\n","\n","        # class and confidence score\n","        print(f\"\\t+ Label: {classes[int(cls_pred)]} | Confidence: {conf.item():0.4f}\")\n","        box_w = x2 - x1\n","        box_h = y2 - y1\n","\n","        # create a detection object and add it to the detection list\n","        detect_data = Detection([x1, y1, box_w, box_h], conf, features)\n","        detection.append(detect_data)\n","        \n","    # update the tracker with the detections\n","    tracker.predict()\n","    tracker.update(detection)\n","\n","    # loop through the tracks and draw bounding boxes and labels on the frame\n","    for track in tracker.tracks:\n","        if not track.is_confirmed() or track.time_since_update > 1:\n","            continue\n","        bbox = track.to_tlwh()\n","        track_id = track.track_id\n","\n","        # draw bounding box\n","        cv2.rectangle(img, (int(bbox[0]), int(bbox[1])),\n","                      (int(bbox[0] + bbox[2]), int(bbox[1] + bbox[3])),\n","                      (0, 255, 0), 2)\n","        # put text with track ID and class label\n","        cv2.putText(img, f\"ID:{track_id}/Conf_Score:{conf:.2f}/Label:{classes[int(cls_pred)]}\",\n","                    (int(bbox[0]), int(bbox[1]) - 10),\n","                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n","    \n","      # Draw bounding boxes around the detected faces\n","        for face in faces:\n","            x, y, width, height = face['box']\n","            cv2.rectangle(img, (x, y), (x+width, y+height), (0, 255, 0), 2)\n","            cv2.putText(img, 'Face', (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n","    #saving results\n","    out.write(img)\n","cv2.destroyAllWindows()\n"," \n"],"metadata":{"id":"Oh4RqCNE86Jw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"cba18ROwE4x3"},"execution_count":null,"outputs":[]}]}